一、数据采集
	1.安装jdk1.8，mysql8.0.31，zookeeper3.7.1，hadoop-ha3.3.4，kafka3.3.1，flink1.17
		基于zookeeper的高可用故障自动转移一定要安装包psmisc（yum -y install psmisc）才能实现
	2.数据采集工作
		所有采集到的数据放到统一目录下作为原始数据
		全量采集：datax
			采集数据量小的数据且几乎不变的数据(虽然每天会采集重复的数据，可根据需求觉决定是否选择覆盖策略)
			使用生成器生成datax任务所需的json文件
			编写自动全量采集的shell脚本
		增量采集：flink任务-flinkcdc，flume1.9.0
			采集数据量大且变化频率高的数据
			开启mysql相应数据库的binlog
			编写flinkcdc程序，提交flink程序到yarn，将mysql增量数据写到kafka对应主题
			编写flume任务的conf文件，从kakfa主题读取数据到HDFS
			编写自动增量采集的shell脚本，mysql->kafka和kafka->hdfs
二、离线数仓
	1.安装hive3.1.3和spark3.3.1-without-hadoop
		这两个版本是不兼容的，但是考虑到稳定性和高性能任然选择了这两个组件版本
		hive3.1.2和hive3.1.3差距也挺大，构建Hive on Spark时，hive3.1.2根本无法找到spark3.3.1的核心包
		hive3.1.3构建Hive on Spark时会报找不到方法异常，使用修改过的源码的hive3.1.3解决这个问题
		尝试自己修改源码，但是搞了很久，环境一直出问题，最后放弃
		Hive on Spark要将spark的jars目录下的jar包上传到hdfs，在hive中配置该相应路径，因为最后要提交到yarn，而yarn时分布式的，要让每个节点都能执行spark程序
	2.构建数仓
		准备工作：根据具体需求构建业务总线矩阵-->参考文档
		构建ods层
			对全量和增量同步的数据分别建立对应的hive表
				注意null值处理，分隔符处理
				注意增量同步数据是json格式，要用hive中对json格式数据建表的方式建表
			将采集到的原始数据加载到hive表中
		构建dim层
			将业务过程涉及到的维度表进行反规范化（就是整合一下维度表）
			相关联的维度表尽可能地整合成一个表放在dim层，减少表的join
				数据量大，变化频率高/数据量小--> 全量周期快照表 
					按日分区，每个分区存放当日全量数据
				数据量大，且变化频率低的--> 拉链表 
					按日分区 两种分区普通日期和9999-12-31分区，前者存放截止到当日过期数据，后者存放目前有效的数据
		构建dwd层
			将需求涉及到的业务过程数据建立相应的事实表
			事实表分类：
				事务事实表  -> 保存最细粒度的操作事件 如下单、支付成功、取消订单
				周期快照事实表 -> 保存存量型指标 需结合相关维度 如每个用户的余额、每个机构的库存
				累积快照事实 -> 保存多个关键业务过程联合处理而构建的事实表 如订单创建到订单完成的过程
		构建dws层
			指标理论：
				原子指标：某一业务过程的度量值 如订单总金额
					三要素：业务过程（如下单），度量值（如金总额），聚合逻辑（sum）
				派生指标：原子指标（聚合函数） + 统计周期（where) + 限定条件(where) + 统计粒度（group by）
				衍生指标：派生指标1 结合 派生指标2
					某机构每天平均运输时长 = 某机构每天运输总时长 / 某机构每天运输总次数 
			将需求中公共的派生指标保存到dws层，做到数据复用，提高查询效率
		构建ads层
			ads层数据只来自dws层
			根据具体的需求统计出相应的指标即可
			一般将需求分成不同的主题，根据主题建立相关的表
				如订单主题，存放的都是和订单相关的需求指标
	3.指标导入到mysql
		在mysql中建立与ads层相同的表
		使用datax将ads层数据导入到mysql（提高可视化展现的效率）
	4.DolphinScheduler自动调度
	5.SuperSet可视化展示
		
	tips：
		构建数仓的dim/dwd层时如果有需要处理新增数据和历史数据的，如分区2024-xx和分区9999-12-31，有两种思路
			思路1：
				将当日最新数据和在生效数据union all连接成一个大表
				通过row number（partition by id order by start_date desc）区分出过期数据和当前生效数据
				关键在于row number，rn = 1的是当前生效数据，rn != 1是过期数据
			思路2：
				先处理当日新过来的数据，该放入9999的放入9999，该放入2024-xx的放入2024-xx
				然后通过union all 9999分区的数据再去处理之前未完成的订单
				关键在于判断分区9999的id是否在新来的数据里面，在里面，说明该条数据有改动，不在里面，说明该条数据没改动
			例子：dim层用户维度表和dwd订单累积快照事实表
		构建数仓dim/dwd层如果遇到首日和每日加载数据的逻辑只有某一个条件不同时可考虑使用op字段，将每日和首日加载用一个脚本
			如dwd成功支付订单表 
				首日加载时订单状态只要不是60010（刚下单）和60999（取消）的，那么都认为是已支付的
				每日加载时订单状态只要是60020（已支付）就是已支付订单，其他状态的不要
					select ...
					from ods_order_info_inc
						where dt = "2024-xx"
						and after.is_deleted = "0"
						and after.status not in ('60010', '60999')
						and (op = 'r' or after.status = '60020')
		
			
三、实时数仓
	1.架构
		不同于离线数仓，实时数仓将kafka作为数据的缓冲中间件，kafka的主题对应相关表格，表格以java实体类转换成json格式字符串的形式写入主题
		mysql--flinkcdc-->kafka(ods)--flink(dwdApp)-->kafka(dwd)--flink(dwsApp)-->kafka(dws)--sink-->Clickhouse(ADS)
							|											↑									|
						flink(dimApp)									|								springboot接口
							↓											|									↓
						 Hbase(dim)  -------------redis-----------------|								suger大屏展示
	2.选型理由
		维度数据存储到Hbase：
			流式处理根据键来查询并关联维度数据，所以应选用k-v类型数据库较为合适，有存储在磁盘的HBase和存储在内存的redis
			而维度数据设计用户基本信息，如用户量很大情况下不应存储在内存中，所以选用redis
		ADS数据存储在clickhouse：
			
			
	2.优化
		问题：关联维度数据条数据都从Hbase查询访问磁盘效率太低，并且要排队等待效率较低
			优化1：旁路缓存
				第一次关联到的维度数据放入redis缓存，之后再有同维度数据关联从redis读取
				写入Hbase的dimApp要实时监控更新的维度数据即操作类型为"u"的维度数据是否在redis中，如果在，则删除
			优化2：多线程异步IO
				关联维度数据采用多线程方式提高效率，Flink中有采用多线程访问HBase的API，但是我们不用，因为我们加了旁路缓存
			
			补充：
				多线程处理时获取线程池的单例设计模式：
					饿汉式：当且仅当类加载时获取 静态代码块
					懒汉式：当且仅当第一次调用获取线程池方法时 synchronized 修饰方法 / synchronized对象锁代码块
						   建议使用synchronized对象锁代码块，因为方法中并非全部代码都是需要锁起来的
				模版方法设计模式：
					在用多线程关联维度数据时需要实现一个接口AsyncFunction	
					可创建一个其子类，如MulThreadCorrelDim将一些通用的代码写入相关方法中，如获取线程，从Hbase/redis获取维度对象，向下游传递数据等
					将一些需要知道具体的数据类型才能做的方法，如关联维度，声明成抽象方法，该类也要声明成抽象类
					具体关联维度的操作到使用的时候创建匿名内部类实现相关方法
					
			







